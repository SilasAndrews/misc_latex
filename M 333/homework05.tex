\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\graphicspath{ {./images/} }




\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\newVec}[2]{\begin{bmatrix}#1 \\ #2 \end{bmatrix}}
\newcommand{\newVect}[3]{\begin{bmatrix}#1 \\ #2 \\ #3\end{bmatrix}}
\newcommand{\newtt}[4]{\begin{bmatrix}#1 & #2\\ #3 & #4 \end{bmatrix}}
\newcommand{\newttt}[9]{\begin{bmatrix}#1 & #2 & #3\\ #4 & #5 & #6 \\ #7 & #8 & #9\end{bmatrix}}
\newcommand{\newdet}[4]{\begin{vmatrix}#1 & #2\\ #3 & #4 \end{vmatrix}}



\title{Linear Algebra Homework 5}
\author{William Jardee}
\date{April 2020}

\begin{document}

\maketitle

\begin{enumerate}


%% Question 1
\item 
\tab Prove Theorem 5.8(d)\par
``If $Q_1$ and $Q_2$ are orthogonal $n \times n$ matrices, then so is $Q_1Q_2$''\\\\
Proof:\par
If $Q_1$ and $Q_2$ are orthogonal, to show that $Q_1Q_2$ is orthogonal, we need to show that $(Q_1Q_2)^{-1} = (Q_1Q_2)^T$. Then the following:
$$(Q_1Q_2)^{-1}=Q_2^{-1} Q_1^{-1} = Q_2^T Q_1^T = (Q_1 Q_2)^T$$
\par
{\raggedleft ``Quack''  $\blacksquare$\\}


%% Question 2
\item
\tab
``Prove that null($A^T$) $\perp$ col($A$)''\\\\
Proof:\par
We must show that if $\textbf{x}\in \mathbb{R}$, then $\textbf{x} \cdot $(every vector in col($A$)) $= 0$ and $\textbf{x}\in$ Null($A^T$) are equivalent. Let us begin $\textbf{x}$ being in Null($A^T$):
\[
\begin{array}{c}
A^T\textbf{x} = \textbf{0}\\
(\textbf{x}^TA)^T = \textbf{0}\\
\textbf{x}^TA = \textbf{0}^T  = \textbf{0}\\
\end{array}
\]
This last line means that $\textbf{x} \cdot($each column of A$) = 0$, or x is perpendicular to each column of A. 
\par
{\raggedleft ``Quack''  $\blacksquare$\\}


%% Question 3
\item
\tab Prove Theorem 5.9(c)\par
``Let W be a subspace of $\mathbb{R}^n$. then $W \cap W^{\perp} = \{\textbf{0}\}$''\\\\
Proof:\par
It is enough to show that if $\textbf{x} \in W$, then Proj$_{W^{\perp}} \textbf{x} = \textbf{0}$. For simplicity we will assume that $W$ and $W^{\perp}$ are basis already. If not we can make both spaces into basis, as the basis will cover the same space and the proof holds.\\
If $\textbf{x} \in W $, then $\textbf{x} $ can be written as 
$$\textbf{x} = c_1 \textbf{w}_1 + c_2 \textbf{w}_2 + ... + c_n \textbf{w}_n$$
Then: Proj$_{W^{\perp}}\textbf{x}$ 
\[
\begin{array}{c}
=(\textbf{x}\cdot\textbf{w}_1^{\perp})\textbf{w}_1^{\perp} + (\textbf{x}\cdot\textbf{w}_2^{\perp})\textbf{w}_2^{\perp} + ... +
(\textbf{x}\cdot\textbf{w}_k^{\perp})\textbf{w}_k^{\perp}\\
=((c_1 \textbf{w}_1 + c_2 \textbf{w}_2 + ... + c_n \textbf{w}_n)\cdot\textbf{w}_1^{\perp})\textbf{w}_1^{\perp} + ...\\
=(c_1 \textbf{w}_1\cdot\textbf{w}_1^{\perp} + c_2 \textbf{w}_2\cdot\textbf{w}_1^{\perp} + ... + c_n \textbf{w}_n\cdot\textbf{w}_1^{\perp})\textbf{w}_1^{\perp} + ...\\
=(c_1 \cdot 0 + c_2 \cdot 0 + ... + c_n \cdot 0)\textbf{w}_1^{\perp} + ...\\
= 0 \cdot \textbf{w}_1^{\perp} + 0 \cdot \textbf{w}_2^{\perp} + ... + 0 \cdot \textbf{w}_k^{\perp} = 0
\end{array}
\]
So the only vector $\textbf{x}$ that can be could be in both $W$ and $W^{\perp}$ is $\textbf{0}$. So the union of the two is $\{\textbf{0}\}$.
\par
{\raggedleft ``Quack''  $\blacksquare$\\}


%% Question 4
\item
Use Gram Schmidt to find an orthonormal basis for the column space of A = 
$\begin{bmatrix}1 & 1 & 0\\ 1 & 1 & 1 \\ 1 & 0 & 1\end{bmatrix}$
\\
\par
$\textbf{v}_1 = \textbf{x}_1 = \newVect{1}{1}{1}$\\
$\textbf{v}_2 = \textbf{x}_2 - \frac{\textbf{v}_1 \cdot \textbf{x}_2}{\textbf{v}_1 \cdot \textbf{v}_1}\textbf{v}_1 = \newVect{1}{1}{0} - \frac{2}{3}\newVect{1}{1}{1} =  \newVect{\frac{1}{3}}{\frac{1}{3}}{-\frac{2}{3}} \rightarrow \newVect{1}{1}{-2}$\\
$\textbf{v}_3 = \textbf{x}_3 - \frac{\textbf{v}_1 \cdot \textbf{x}_3}{\textbf{v}_1 \cdot \textbf{v}_1}\textbf{v}_1 - \frac{\textbf{v}_2 \cdot \textbf{x}_3}{\textbf{v}_2 \cdot \textbf{v}_2}\textbf{v}_2 = \newVect{0}{1}{1} - \frac{4}{6}\newVect{1}{1}{1} + \frac{1}{6}\newVect{1}{1}{-2}$\\
\tab$=\newVect{0}{1}{1} - \newVect{\frac{3}{6}}{\frac{3}{6}}{1} = \newVect{-\frac{1}{2}}{\frac{1}{2}}{0} \rightarrow \newVect{-1}{1}{0}$\\\\\\
Orthonormal basis for $A = $ Span
$\left( \newVect{\frac{1}{\sqrt{3}}}{\frac{1}{\sqrt{3}}}{\frac{1}{\sqrt{3}}}, \newVect{\frac{1}{\sqrt{6}}}{\frac{1}{\sqrt{6}}}{-\frac{2}{\sqrt{6}}}, \newVect{-\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}}{0}\right)$.
\par
\newpage

%% Question 5
\item
$A = \newttt{1}{1}{1}{1}{1}{1}{1}{1}{1}$
\par
\begin{enumerate}
    \item Prove that A is orthogonally diagonalizable.\\
        All symmetric matrices are orthogonally diagonalizable, so A is.\\
    \item Orthongonally diagonalize A.\\
    To first look for the eigenvalues:\\
$0 = (1-\lambda)\newdet{1-\lambda}{1}{1}{1-\lambda} - \newdet{1}{1}{1}{1-\lambda} + \newdet{1}{1}{1-\lambda}{1}$\\
$= (1-\lambda)[(1-\lambda)(1-\lambda)-1] - ((1-\lambda)-1) +(1-(1-\lambda))$\\
$= (1-\lambda)^3 - (1-\lambda) - (1-\lambda) + 1 + 1 - (1 - \lambda)$\\ 
$= (1-\lambda)(1-2\lambda+\lambda^2) - 3 + 3\lambda +2$\\
$= (1 - 2\lambda + \lambda^2 - \lambda + 2\lambda^2 - \lambda^3 - 1 + 3\lambda$\\
$= -\lambda^3 + 3\lambda^2$\\\\
\tab $\lambda = 0,$ with mult. 2 and  $\lambda = 3$\\

\tab Solving for $\lambda = 0$:\\
$\newttt{1}{1}{1}{1}{1}{1}{1}{1}{1}\textbf{x} = \textbf{0}$\\
$x+y+z=0$ \tab $\rightarrow$ \tab 
$\newVect{-y-z}{y}{z} = y\newVect{-1}{1}{0}+z\newVect{-1}{0}{1}$\\\\\\

\tab Solving for $\lambda = 3$:\\
$\newttt{-2}{1}{1}{1}{-2}{1}{1}{1}{-2}\textbf{x} = \textbf{0}$\\
\[\begin{array}{c}
\begin{bmatrix}-2&1&1&0\\1&-2&1&0\\1&1&-2&0\end{bmatrix}
=\begin{bmatrix}-2&1&1&0\\1&-2&1&0\\0&3&-3&0\end{bmatrix}
=\begin{bmatrix}-2&1&1&0\\1&-1&0&0\\0&1&-1&0\end{bmatrix}\\
=\begin{bmatrix}0&-1&1&0\\1&-1&0&0\\0&1&-1&0\end{bmatrix}
=\begin{bmatrix}1&-1&0&0\\0&-1&1&0\\0&0&0&0\end{bmatrix}
=\begin{bmatrix}1&0&-1&0\\0&-1&1&0\\0&0&0&0\end{bmatrix}
\end{array}\]\\
So the related eigenvector is $\newVect{1}{1}{1}$\\
$$A = \newttt{1}{1}{1}{1}{1}{1}{1}{1}{1} = \newttt{\frac{1}{\sqrt{3}}}{\frac{1}{\sqrt{3}}}{\frac{1}{\sqrt{3}}}{-\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{2}}}{0}{-\frac{1}{\sqrt{2}}}{0}{\frac{1}{\sqrt{2}}}\newttt{3}{0}{0}{0}{0}{0}{0}{0}{0}
\newttt{\frac{1}{\sqrt{3}}}{-\frac{1}{\sqrt{2}}}{-\frac{1}{\sqrt{2}}}{\frac{1}{\sqrt{3}}}{\frac{1}{\sqrt{2}}}{0}{\frac{1}{\sqrt{3}}}{0}{\frac{1}{\sqrt{2}}}$$
\end{enumerate}
\par
\newpage


%% Question 6
\item
$A = \newttt{4}{-1}{-2}{2}{1}{-2}{1}{-1}{1}$
\begin{enumerate}
    \item Prove that A is not orthogonally diagonalizable.\\
    The matrix is not symmetric, so it is not orthogonally diagonalizable.
    
    \item Prove that A is diagonalizable.\\
    To prove that the matrix is diagonalizable, it must be shown that the algebraic multiplicity of the eigenvalues is equal to their geometric multiplicity.\\
    $0 = (4-\lambda)\newdet{1-\lambda}{-2}{-1}{1-\lambda} - 2\newdet{-1}{-2}{-1}{1-\lambda} + \newdet{-1}{-2}{1-\lambda}{-2}$\\
    $= (4-\lambda)((1-\lambda)^2-2)-2(-(1-\lambda)-2) + (2+2(1-\lambda))$\\
    $= (4-\lambda)(1-2\lambda+\lambda^2 -2) -2(-3 + \lambda) + (4 - 2\lambda)$\\
    $= (-\lambda+4)(\lambda^2 -2\lambda - 1) - 2\lambda + 6 + 4 - 2\lambda$\\
    $= -\lambda^3 + 2\lambda^2 + \lambda + 4\lambda^2 -8\lambda - 4 -4\lambda + 10$\\
    $= -\lambda^3 + 6\lambda^2 -11\lambda + 6$\\
    $= (\lambda -1)(\lambda-2)(\lambda-3)$\\
    $$\lambda = 1,2,3$$
    Each eigenvalue has a algebraic multiplicity of 1, and each one has a geometric multiplicity of at least 1, and the total number of vectors is 3. Since each algebraic multiplicity is equal to their geometric multiplicity, the matrix is diagonalizable.
    
    \item
    For $\lambda = 1$:\\\\
    $\newttt{3}{-1}{-2}{2}{0}{-2}{1}{-1}{0}\textbf{x} = \textbf{0}$\\
    $2x-2z = 0$, and $x-y = 0$, so the eigenvector, $\textbf{v}_1 = \newVect{1}{1}{1}$\\\\
    
    For $\lambda = 2$:\\\\
    $\newttt{2}{-1}{-2}{2}{-1}{-2}{1}{-1}{-1}\textbf{x} = \textbf{0}$\\
    $\begin{bmatrix}
    2&-1&-2&0\\
    1&-1&-1&0\\
    0&0&0&0
    \end{bmatrix} = 
    \begin{bmatrix}
    0&1&0&0\\
    1&-1&-1&0\\
    0&0&0&0
    \end{bmatrix}$\\
    So the eigenvector $\textbf{v}_2 = \newVect{1}{0}{1}$\\\\
    
    For $\lambda = 3$:\\\\
    $\newttt{1}{-1}{-2}{2}{-2}{-2}{1}{-1}{-2}\textbf{x} = \textbf{0}$\\
    $\begin{bmatrix}
    1&-1&-2&0\\
    2&-2&-2&0\\
    0&0&0&0
    \end{bmatrix} = 
    \begin{bmatrix}
    1&-1&-2&0\\
    0&0&2&0\\
    0&0&0&0
    \end{bmatrix}$\\
    So the eigenvector $\textbf{v}_3 = \newVect{1}{1}{0}$\\\\
    
    $A = \newttt{4}{-1}{-2}{2}{1}{-2}{1}{-1}{1}=\newttt{1}{1}{1}{1}{0}{1}{1}{1}{0}
    \newttt{1}{0}{0}{0}{2}{0}{0}{0}{3}\newttt{1}{1}{1}{1}{0}{1}{1}{1}{0}^{-1}$\\\\
    det$\left(\newttt{1}{1}{1}{1}{0}{1}{1}{1}{0}\right)$ $= -1+1+1=1$\\
    $\newttt{1}{1}{1}{1}{0}{1}{1}{1}{0}^{-1} = \frac{1}{1}\newttt{-1}{1}{1}{1}{-1}{0}{1}{0}{-1}$\\\\\\
    $A = \newttt{4}{-1}{-2}{2}{1}{-2}{1}{-1}{1}=\newttt{1}{1}{1}{1}{0}{1}{1}{1}{0}
    \newttt{1}{0}{0}{0}{2}{0}{0}{0}{3}\newttt{-1}{1}{1}{1}{-1}{0}{1}{0}{-1}$\\\\
\end{enumerate}
\par
\newpage

%% Question 7
\item 
$A = \newttt{3}{-1}{0}{0}{3}{0}{0}{-1}{3}$
\begin{enumerate}
    \item Prove that A is not orthogonally diagonalizable\\
    A is not symmetric, so A is not orthogonally diagonalizable
    \item Prove that A is not diagonalizable.\\
    To show A is not diagonalizable, the geometric multiplicity is not equal to the algebraic multiplicity.\\
    $0 = $det$(A) = (3-\lambda)(3-\lambda)(3-\lambda)$\\
    So $\lambda = 3$, with multiplicity 3.\\\\
    \tab Checking for the geometric multiplicity:\\\\
    $\newttt{0}{-1}{0}{0}{0}{0}{0}{-1}{0}\textbf{x} = \textbf{0}$, then $\textbf{x} = x\newVect{1}{0}{0} + y\newVect{0}{0}{1}$.\\
    $\textbf{v}_1 = \newVect{1}{0}{0}$, $\textbf{v}_2 = \newVect{0}{0}{1}$.\\\\\\
    So the algebraic multiplicity of $\lambda = 3$ is 2, but the geometric multiplicity is 2; so the matrix A is not diagonalizable.\\
    \item Prove that A is orthogonally triangularizable.\\
    For the matrix to be orthogonally triangularizable, the matrix has to have real eigenvalues. All the eigenvalues are 3, which is real, so the matrix A is orthogonally triangularizable.\\
    \item To get the orthogonal matrix, the eigenvectors make up the first two columns. To complete the orthonormal basis the last orthogonal vector can be used, $\newVect{0}{1}{0}$. So 
    $$Q =\newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}$$
    To verify Q is orthogonal, $Q^TQ= I$. This is obviously true.\\
    To get the Triangular matrix, $T = Q^TAQ$.
    \newpage
    $\newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}\newttt{3}{0}{0}{-1}{3}{-1}{0}{0}{3}
     \newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}$\\
    $= \newttt{3}{-1}{0}{0}{-1}{3}{0}{3}{0}\newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}$\\
    $= \newttt{3}{0}{-1}{0}{3}{-1}{0}{0}{3} = T$\\\\\\
    So $A = \newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}\newttt{3}{0}{-1}{0}{3}{-1}{0}{0}{3}
            \newttt{1}{0}{0}{0}{0}{1}{0}{1}{0}$\\
\end{enumerate}


%% Question 8
\item \tab 
``Let A be an $n\times n$ real matrix, all of whose eigenvalues are real. Prove that there exist an orthogonal matrix $Q$ and an upper triangular matrix T such that $Q^TAQ = T$.''\\\\
Proof by induction:\par
Take the base case as the $1\times 1$ works, as any $1 \times 1$ matrix is already a orthogonally triangularizable with $Q = [1]$.\\\\
For the inductive step, assume the $k\times k$ matrix $A$ is able to be written as $Q^TAQ = T$. Then for the $(k+1) \times (k+1)$, take the real eigenvalue be $\lambda_1$, then $\textbf{v}_1$ is a real eigenvector related to $\lambda_1$, such that $\textbf{v}_1$ is a unit vector.\par
Use the Gram-Schmit method to develop an orthogonal basis such that 
$$Q = [\textbf{v}_1, \textbf{v}_2, \cdots, \textbf{v}_{k+1}]$$
$Q^TAQ =
\begin{bmatrix}
\textbf{v}_1^T\\\textbf{v}_2^T\\\cdots\\\textbf{v}_{k+1}^T
\end{bmatrix}A
\begin{bmatrix}
\textbf{v}_1 & \textbf{v}_2 & \cdots & \textbf{v}_{k+1}
\end{bmatrix} = 
\begin{bmatrix}
\textbf{v}_1^T\\\textbf{v}_2^T\\\cdots\\\textbf{v}_{k+1}^T
\end{bmatrix}
\begin{bmatrix}
\lambda_1\textbf{v}_1 & A\textbf{v}_2 & \cdots & A\textbf{v}_{k+1}
\end{bmatrix}$\\
$=\begin{bmatrix}
\lambda_1 &  *\\
0         & B_{k\times k}
\end{bmatrix}$\\
Such that $B_{k\times k} = \begin{bmatrix}
\textbf{v}_2^T\\\cdots\\\textbf{v}_{k+1}^T
\end{bmatrix}A
\begin{bmatrix}
\textbf{v}_2 & \cdots & \textbf{v}_{k+1}
\end{bmatrix} = V^TAV$, where $V$ is an orthogonal matrix.\\
So now, $Q^TAQ = \begin{bmatrix}
\lambda_1 &  *\\
0         & V^TAV
\end{bmatrix}=
\begin{bmatrix}
\lambda_1 &  *\\
0         & T
\end{bmatrix}$\\
\newpage
So for any $(k+1)\times (k+1)$ matrix $A$, $Q^TAT = T'$, where $T'$ is an upper triangular matrix. We can generalize this to any $n\times n$ matrix $A$ can be written as $Q^TAQ = T$.
\par
{\raggedleft ``Quack''  $\blacksquare$\\}


%% Question 9
\item \tab ``Let $A = QTQ^T$ be the Schur Triangulation of a square matrix $A$, where $Q$ is orthogonal and $T$ is upper triangular. Prove that the eigenvalues of $A$ are on the diagonal of $T$.''\\\\
Proof:\\

$0 =$ det$(A - \Lambda)$
 = det$(QTQ^T - \lambda QQ^T)$\\
 = det$(QTQ^T - Q\lambda IQ^T)$
= det$(Q(T- \lambda I)Q^T)$\\
= det$(Q)$det$(T-\lambda I)$det$(Q^T)$\\
= det$(Q)$det$(Q^{-1})$det$(T-\lambda I)$\\
= det$(T - \lambda I)$\\\\
Since $A$ and $T$ have the same characteristic equation, they have the same eigenvalues. $T$ is a triangular matrix, so the eigenvalues are on the diagonal. So it follows that the eigenvalues of $A$ are on the diagonal of $T$.
\par
{\raggedleft ``Quack''  $\blacksquare$\\}


%% Question 10
\item \tab
``Let $A = QTQ^T$ be the Schur Triangulation of a square matrix $A$, where $Q$ is orthogonal and $T$ is upper triangular. Prove that if $A$ is invertible, then $A^{-1} = QT^{-1}Q^T$.''\\\\
Proof:\\
If $A = QTQ^T$ and $A$ is invertible, then:
$$A^{-1} = (QTQ^T)^{-1} = (Q^T)^{-1}T^{-1}Q^{-1} = QT^{-1}Q^T$$
This assumes that $T^{-1}$ exists. We can verify this assumption by seeing that $A$ and $T$ have the same eigenvalues, using the result from Question 9, and consequently the same determinants. So, because $A$ is invertible, det$(A)$ is not 0, so det$(T)$ is not 0, and we can see $T$ is invertible.
\par
{\raggedleft ``Quack''  $\blacksquare$\\}



\end{enumerate}

\end{document}